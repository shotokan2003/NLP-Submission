{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w9yG1wK9EXq"
      },
      "outputs": [],
      "source": [
        "##create a transformer from scratch using pytorch library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Multi-Head Attention module\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by the number of heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(embed_size, embed_size)\n",
        "        self.value = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        N = queries.shape[0]\n",
        "        Q = self.query(queries)\n",
        "        K = self.key(keys)\n",
        "        V = self.value(values)\n",
        "\n",
        "        # Split into heads\n",
        "        Q = Q.view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(N, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        energy = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "          energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
        "\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        out = torch.matmul(attention, V)\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(N, -1, self.num_heads * self.head_dim)\n",
        "\n",
        "        # Final linear layer\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "# Position-wise Feed-Forward Network\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, ff_dim):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.encoding[:, :seq_len, :].to(x.device)\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, ff_dim, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = PositionWiseFeedForward(embed_size, ff_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self-attention\n",
        "        attn_out = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "# Full Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, ff_dim, num_layers, vocab_size, max_len, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_size, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x) * math.sqrt(self.embed_size)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    embed_size = 512\n",
        "    num_heads = 8\n",
        "    ff_dim = 2048\n",
        "    num_layers = 6\n",
        "    vocab_size = 10000\n",
        "    max_len = 100\n",
        "    dropout = 0.1\n",
        "\n",
        "    model = TransformerEncoder(embed_size, num_heads, ff_dim, num_layers, vocab_size, max_len, dropout)\n",
        "\n",
        "    src = torch.randint(0, vocab_size, (32, max_len))  # (batch_size, seq_len)\n",
        "    # src_mask = torch.ones((32, max_len, max_len))  # Example mask\n",
        "    # Example mask with appropriate shape\n",
        "    src_mask = torch.ones((32, 1, max_len, max_len))  # Add an extra dimension for num_heads\n",
        "\n",
        "\n",
        "    out = model(src, src_mask)\n",
        "    print(out.shape)  # Should be (32, max_len, embed_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUv1wN7v949l",
        "outputId": "8f7c4cf7-e2ad-4854-8e0c-1c48205e7fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 512])\n"
          ]
        }
      ]
    }
  ]
}